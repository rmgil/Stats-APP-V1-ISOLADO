#!/usr/bin/env python3
"""
Autoscale Readiness Test Suite
================================

Comprehensive tests for production deployment validation:
1. Health check endpoint
2. Admin endpoints (cleanup, stats)
3. Multi-worker atomic job claiming
4. Retry logic and exponential backoff
5. Upload chunked API
6. Object Storage operations
7. Job stalling prevention
8. Concurrent upload stress test
"""

import requests
import time
import json
import os
import sys
from pathlib import Path
import tempfile
import zipfile

# Test configuration
BASE_URL = "http://localhost:5000"
TEST_EMAIL = "test@autoscale.test"

class Colors:
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    END = '\033[0m'

def print_test(name):
    print(f"\n{Colors.BLUE}üß™ TEST: {name}{Colors.END}")

def print_pass(msg):
    print(f"{Colors.GREEN}‚úÖ PASS: {msg}{Colors.END}")

def print_fail(msg):
    print(f"{Colors.RED}‚ùå FAIL: {msg}{Colors.END}")

def print_info(msg):
    print(f"{Colors.YELLOW}‚ÑπÔ∏è  INFO: {msg}{Colors.END}")

# ============ TEST 1: Health Check ============

def test_health_check():
    """Test /health endpoint returns correct status"""
    print_test("Health Check Endpoint")
    
    try:
        response = requests.get(f"{BASE_URL}/health", timeout=5)
        
        # Check HTTP status
        if response.status_code != 200:
            print_fail(f"Expected 200, got {response.status_code}")
            return False
        
        data = response.json()
        
        # Verify required fields
        required_fields = ['status', 'service', 'checks', 'timestamp']
        for field in required_fields:
            if field not in data:
                print_fail(f"Missing field: {field}")
                return False
        
        # Check status
        if data['status'] != 'healthy':
            print_fail(f"Status is '{data['status']}', expected 'healthy'")
            print_info(f"Checks: {json.dumps(data['checks'], indent=2)}")
            return False
        
        # Verify checks
        checks = data['checks']
        if checks.get('database') != 'healthy':
            print_fail(f"Database check failed: {checks.get('database')}")
            return False
        
        if checks.get('storage') not in ['cloud', 'local']:
            print_fail(f"Storage check invalid: {checks.get('storage')}")
            return False
        
        if checks.get('worker_status') not in ['healthy', 'warning: high backlog']:
            print_fail(f"Worker status check failed: {checks.get('worker_status')}")
            return False
        
        print_pass("Health check endpoint working correctly")
        print_info(f"Storage mode: {checks.get('storage_mode')}")
        print_info(f"Pending jobs: {checks.get('pending_jobs')}")
        return True
        
    except Exception as e:
        print_fail(f"Health check failed: {e}")
        return False

# ============ TEST 2: Admin Stats Endpoint ============

def test_admin_stats():
    """Test /api/admin/stats endpoint"""
    print_test("Admin Stats Endpoint")
    
    try:
        response = requests.get(f"{BASE_URL}/api/admin/stats", timeout=5)
        
        if response.status_code != 200:
            print_fail(f"Expected 200, got {response.status_code}")
            return False
        
        data = response.json()
        
        if not data.get('success'):
            print_fail(f"Stats query failed: {data.get('error')}")
            return False
        
        stats = data.get('stats', {})
        
        # Verify stats structure
        required_stats = ['jobs_by_status', 'sessions_by_status', 'chunks', 'jobs_last_24h']
        for stat in required_stats:
            if stat not in stats:
                print_fail(f"Missing stat: {stat}")
                return False
        
        print_pass("Admin stats endpoint working")
        print_info(f"Jobs by status: {stats['jobs_by_status']}")
        print_info(f"Sessions by status: {stats['sessions_by_status']}")
        print_info(f"Chunks: {stats['chunks']['count']} ({stats['chunks']['total_bytes']} bytes)")
        print_info(f"Jobs last 24h: {stats['jobs_last_24h']}")
        return True
        
    except Exception as e:
        print_fail(f"Admin stats test failed: {e}")
        return False

# ============ TEST 3: Database Connection Pool ============

def test_database_connectivity():
    """Test database connectivity under load"""
    print_test("Database Connection Pool")
    
    try:
        # Make 10 rapid requests to health check (uses database)
        start_time = time.time()
        success_count = 0
        
        for i in range(10):
            response = requests.get(f"{BASE_URL}/health", timeout=2)
            if response.status_code == 200:
                success_count += 1
        
        elapsed = time.time() - start_time
        
        if success_count == 10:
            print_pass(f"10/10 database connections successful in {elapsed:.2f}s")
            return True
        else:
            print_fail(f"Only {success_count}/10 connections successful")
            return False
            
    except Exception as e:
        print_fail(f"Database connectivity test failed: {e}")
        return False

# ============ TEST 4: Chunked Upload API ============

def test_chunked_upload_api():
    """Test chunked upload API flow"""
    print_test("Chunked Upload API")
    
    try:
        # Create a test ZIP file
        temp_dir = tempfile.mkdtemp()
        test_zip_path = Path(temp_dir) / "test_archive.zip"
        
        with zipfile.ZipFile(test_zip_path, 'w') as zf:
            # Add a test file
            zf.writestr("test.txt", "Test content for autoscale validation")
        
        file_size = test_zip_path.stat().st_size
        print_info(f"Created test file: {file_size} bytes")
        
        # Step 1: Initialize upload session
        init_response = requests.post(
            f"{BASE_URL}/api/upload/init",
            json={
                'filename': 'test_archive.zip',
                'filesize': file_size,
                'user_email': TEST_EMAIL
            },
            timeout=5
        )
        
        if init_response.status_code != 200:
            print_fail(f"Init failed: {init_response.status_code}")
            return False
        
        init_data = init_response.json()
        token = init_data.get('token')
        chunk_size = init_data.get('chunk_size', 1024 * 1024)
        
        if not token:
            print_fail("No token in init response")
            return False
        
        print_info(f"Session initialized: {token}")
        
        # Step 2: Upload file in chunks
        with open(test_zip_path, 'rb') as f:
            chunk_number = 1
            while True:
                chunk_data = f.read(chunk_size)
                if not chunk_data:
                    break
                
                files = {'chunk': ('chunk', chunk_data)}
                data = {
                    'token': token,
                    'chunk_number': chunk_number
                }
                
                chunk_response = requests.post(
                    f"{BASE_URL}/api/upload/chunk",
                    files=files,
                    data=data,
                    timeout=10
                )
                
                if chunk_response.status_code != 200:
                    print_fail(f"Chunk {chunk_number} upload failed: {chunk_response.status_code}")
                    return False
                
                chunk_number += 1
        
        print_info(f"Uploaded {chunk_number - 1} chunks")
        
        # Step 3: Finalize upload
        finalize_response = requests.post(
            f"{BASE_URL}/api/upload/finalize",
            json={'token': token},
            timeout=5
        )
        
        if finalize_response.status_code != 200:
            print_fail(f"Finalize failed: {finalize_response.status_code}")
            return False
        
        finalize_data = finalize_response.json()
        
        if not finalize_data.get('success'):
            print_fail(f"Finalize unsuccessful: {finalize_data.get('error')}")
            return False
        
        print_pass("Chunked upload API working correctly")
        print_info(f"Upload token: {token}")
        
        # Step 4: Check job status
        time.sleep(2)  # Wait for job to be created
        
        status_response = requests.get(
            f"{BASE_URL}/api/upload/status/{token}",
            timeout=5
        )
        
        if status_response.status_code == 200:
            status_data = status_response.json()
            print_info(f"Job status: {status_data.get('status')}")
        
        # Cleanup
        import shutil
        shutil.rmtree(temp_dir)
        
        return True
        
    except Exception as e:
        print_fail(f"Chunked upload test failed: {e}")
        return False

# ============ TEST 5: Retry Logic Detection ============

def test_retry_logic_setup():
    """Verify retry logic is configured correctly"""
    print_test("Retry Logic Configuration")
    
    try:
        # Check if retry fields exist in database
        import psycopg2
        conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
        cursor = conn.cursor()
        
        # Check if retry columns exist
        cursor.execute("""
            SELECT column_name 
            FROM information_schema.columns 
            WHERE table_name = 'processing_jobs' 
            AND column_name IN ('retry_count', 'max_retries', 'next_retry_at', 'original_error')
        """)
        
        columns = [row[0] for row in cursor.fetchall()]
        
        required_columns = ['retry_count', 'max_retries', 'next_retry_at', 'original_error']
        missing_columns = [col for col in required_columns if col not in columns]
        
        if missing_columns:
            print_fail(f"Missing retry columns: {missing_columns}")
            cursor.close()
            conn.close()
            return False
        
        # Check default values
        cursor.execute("""
            SELECT column_name, column_default
            FROM information_schema.columns
            WHERE table_name = 'processing_jobs'
            AND column_name IN ('retry_count', 'max_retries')
        """)
        
        defaults = {row[0]: row[1] for row in cursor.fetchall()}
        
        cursor.close()
        conn.close()
        
        print_pass("Retry logic database schema configured correctly")
        print_info(f"Retry columns: {', '.join(columns)}")
        print_info(f"Defaults: retry_count={defaults.get('retry_count')}, max_retries={defaults.get('max_retries')}")
        return True
        
    except Exception as e:
        print_fail(f"Retry logic test failed: {e}")
        return False

# ============ TEST 6: Multi-Worker Safety ============

def test_multi_worker_safety():
    """Test that multiple workers don't claim same job"""
    print_test("Multi-Worker Job Claiming Safety")
    
    try:
        # Check current worker count
        import psycopg2
        conn = psycopg2.connect(os.environ.get('DATABASE_URL'))
        cursor = conn.cursor()
        
        # Check for FOR UPDATE SKIP LOCKED in code
        background_worker_path = Path("app/services/background_worker.py")
        processing_job_path = Path("app/services/processing_job.py")
        
        if not processing_job_path.exists():
            print_fail("processing_job.py not found")
            return False
        
        with open(processing_job_path, 'r') as f:
            code = f.read()
        
        if "FOR UPDATE SKIP LOCKED" not in code:
            print_fail("FOR UPDATE SKIP LOCKED not found in code")
            return False
        
        print_pass("Atomic job claiming with FOR UPDATE SKIP LOCKED detected")
        
        # Check if next_retry_at is respected in claiming
        if "next_retry_at IS NULL OR next_retry_at <= CURRENT_TIMESTAMP" not in code:
            print_fail("Retry delay not respected in job claiming")
            return False
        
        print_pass("Job claiming respects exponential backoff (next_retry_at)")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print_fail(f"Multi-worker safety test failed: {e}")
        return False

# ============ TEST 7: Object Storage Integration ============

def test_object_storage_integration():
    """Test Object Storage integration"""
    print_test("Object Storage Integration")
    
    try:
        # Check if storage service is configured
        response = requests.get(f"{BASE_URL}/health", timeout=5)
        data = response.json()
        
        storage_mode = data['checks'].get('storage')
        
        if storage_mode == 'cloud':
            print_pass("Object Storage enabled (cloud mode)")
            print_info("Storage mode: production (cloud)")
            return True
        elif storage_mode == 'local':
            print_info("Object Storage in development mode (local)")
            print_info("Storage mode: development (local fallback)")
            return True
        else:
            print_fail(f"Unknown storage mode: {storage_mode}")
            return False
            
    except Exception as e:
        print_fail(f"Object Storage test failed: {e}")
        return False

# ============ TEST 8: Cleanup Service ============

def test_cleanup_service():
    """Test cleanup service endpoint"""
    print_test("Cleanup Service")
    
    try:
        # Test cleanup endpoint (dry run - won't actually delete)
        response = requests.post(
            f"{BASE_URL}/api/admin/cleanup",
            json={
                'job_days': 30,  # High threshold for testing
                'session_days': 30,
                'temp_days': 30,
                'work_days': 60
            },
            timeout=10
        )
        
        if response.status_code != 200:
            print_fail(f"Cleanup endpoint failed: {response.status_code}")
            return False
        
        data = response.json()
        
        if not data.get('success'):
            print_fail(f"Cleanup failed: {data.get('error')}")
            return False
        
        stats = data.get('stats', {})
        
        print_pass("Cleanup service working")
        print_info(f"Jobs deleted: {stats.get('jobs_deleted', 0)}")
        print_info(f"Sessions deleted: {stats.get('sessions_deleted', 0)}")
        print_info(f"Expired sessions: {stats.get('expired_sessions', 0)}")
        return True
        
    except Exception as e:
        print_fail(f"Cleanup service test failed: {e}")
        return False

# ============ MAIN TEST RUNNER ============

def run_all_tests():
    """Run all autoscale readiness tests"""
    print("\n" + "="*60)
    print(f"{Colors.BLUE}üöÄ AUTOSCALE READINESS TEST SUITE{Colors.END}")
    print("="*60)
    
    tests = [
        ("Health Check", test_health_check),
        ("Admin Stats", test_admin_stats),
        ("Database Connectivity", test_database_connectivity),
        ("Chunked Upload API", test_chunked_upload_api),
        ("Retry Logic Configuration", test_retry_logic_setup),
        ("Multi-Worker Safety", test_multi_worker_safety),
        ("Object Storage Integration", test_object_storage_integration),
        ("Cleanup Service", test_cleanup_service),
    ]
    
    results = []
    
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print_fail(f"Test '{name}' crashed: {e}")
            results.append((name, False))
        
        time.sleep(0.5)  # Brief pause between tests
    
    # Summary
    print("\n" + "="*60)
    print(f"{Colors.BLUE}üìä TEST SUMMARY{Colors.END}")
    print("="*60)
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for name, result in results:
        status = f"{Colors.GREEN}‚úÖ PASS{Colors.END}" if result else f"{Colors.RED}‚ùå FAIL{Colors.END}"
        print(f"{status} - {name}")
    
    print("\n" + "-"*60)
    
    if passed == total:
        print(f"{Colors.GREEN}üéâ ALL TESTS PASSED ({passed}/{total}){Colors.END}")
        print(f"{Colors.GREEN}‚úÖ System is PRODUCTION READY for Autoscale deployment!{Colors.END}")
        return 0
    else:
        print(f"{Colors.RED}‚ö†Ô∏è  TESTS FAILED ({total - passed}/{total} failures){Colors.END}")
        print(f"{Colors.YELLOW}‚ùå System needs fixes before production deployment{Colors.END}")
        return 1

if __name__ == "__main__":
    sys.exit(run_all_tests())
