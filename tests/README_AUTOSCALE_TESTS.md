# Autoscale Production Tests

Testes de produÃ§Ã£o para validar o sistema de upload distribuÃ­do com autoscale deployment.

## ğŸ¯ Objetivos

Validar que o sistema suporta **8-12 uploads simultÃ¢neos de 100MB+** sem:
- Falhas de race conditions
- Exhaustion de connection pool
- Memory leaks
- Timeouts ou freezes

---

## ğŸ“‹ PrÃ©-requisitos

### 1. Instalar DependÃªncias de Teste

```bash
pip install aiohttp psutil
```

### 2. Configurar Ambiente

Definir variÃ¡veis de ambiente (opcional - defaults para localhost):

```bash
export BASE_URL="http://localhost:5000"
export TEST_USER_EMAIL="test@example.com"
export TEST_USER_PASSWORD="testpass"
```

### 3. Sistema Em ExecuÃ§Ã£o

Garantir que:
- Flask server estÃ¡ running (local ou production)
- PostgreSQL database acessÃ­vel
- Background worker ativo

---

## ğŸ§ª Testes DisponÃ­veis

### Task 18: Regression Test - SERIALIZABLE Validation

**Objetivo:** Validar que 2 uploads podem finalizar simultaneamente sem race conditions.

**O que testa:**
- âœ“ SERIALIZABLE transaction isolation
- âœ“ Safeguards atÃ³micos (queue + user limits)
- âœ“ Retry logic em serialization conflicts
- âœ“ Autocommit preservation no connection pool

**Como executar:**

```bash
# 1. Criar ficheiros de teste (pequenos para este teste)
mkdir -p tests/fixtures
dd if=/dev/urandom of=tests/fixtures/test_file_1.zip bs=1M count=10
dd if=/dev/urandom of=tests/fixtures/test_file_2.zip bs=1M count=10

# 2. Executar teste
python tests/test_concurrent_uploads.py
```

**Resultado esperado:**
```
âœ“âœ“ PASS: Both uploads finalized successfully!
âœ“ SERIALIZABLE transactions working correctly
âœ“ No race conditions detected
```

---

### Task 19: Stress Test - 12 Concurrent 100MB Uploads

**Objetivo:** Testar limites do sistema com carga mÃ¡xima.

**O que testa:**
- âœ“ Connection pool (20 conns/worker) nÃ£o esgota
- âœ“ Memory streaming (fetchmany) previne overflow
- âœ“ Queue safeguards bloqueiam quando >= 50 pending
- âœ“ 8 workers Gunicorn processam tudo

**Como executar:**

```bash
# Executar teste (cria ficheiros automaticamente)
python tests/test_stress_uploads.py
```

**MÃ©tricas monitorizadas:**
- Total time
- Throughput (MB/s)
- Peak memory usage
- Success/failure rate

**Resultado esperado:**
```
âœ“âœ“âœ“ PASS: All uploads completed successfully!
âœ“ Connection pool handled concurrent load
âœ“ Memory usage acceptable
âœ“ Queue safeguards working
```

---

### Task 20: Soak Test - 50 Uploads Over 2 Hours

**Objetivo:** Detectar memory leaks e instabilidade ao longo do tempo.

**O que testa:**
- âœ“ Memory leaks (trend analysis)
- âœ“ Connection pool nÃ£o exaure ao longo do tempo
- âœ“ Worker heartbeat + timeout funcionam
- âœ“ GC enforcement evita acumulaÃ§Ã£o

**Como executar:**

```bash
# Executar teste (2+ horas de runtime)
python tests/test_soak.py
```

**MÃ©tricas monitorizadas:**
- Memory delta (initial â†’ final)
- Memory leak rate (MB/hour)
- Upload time stability
- Failure pattern over time

**Resultado esperado:**
```
âœ“âœ“âœ“ PASS: All uploads successful!
âœ“ No memory leaks detected
âœ“ System stable over 2 hours
```

---

## ğŸ“Š InterpretaÃ§Ã£o de Resultados

### âœ… Sistema PASS se:

1. **Task 18:**
   - Ambos uploads finalizam com sucesso
   - Nenhum erro de "serialization conflict"
   - Safeguards bloqueiam corretamente

2. **Task 19:**
   - Todos 12 uploads completam (0 failures)
   - Peak memory < 2GB (com 100MB x 12)
   - Throughput estÃ¡vel (sem degradaÃ§Ã£o)

3. **Task 20:**
   - Memory leak rate < 10 MB/hour
   - 0-2 failures mÃ¡ximo (< 5%)
   - Upload times consistentes (Â±20% variance)

### âŒ Sistema FAIL se:

- **Serialization errors frequentes** â†’ Connection pool contamination
- **Memory > 3GB** â†’ Streaming nÃ£o estÃ¡ funcionando
- **> 3 failures em Task 19** â†’ Race conditions ou safeguards broken
- **Memory leak > 50 MB/hour** â†’ GC nÃ£o estÃ¡ enforcing

---

## ğŸ” Debugging Failures

### 1. Check Worker Status

```bash
curl http://localhost:5000/api/worker/status
```

Verificar:
- `health: 'healthy'` (ou 'degraded'/'critical')
- `queue.pending` < 50
- `workers.active` > 0
- `latest_heartbeat` recente

### 2. Check Database

```bash
psql $DATABASE_URL -c "
SELECT status, COUNT(*) 
FROM processing_jobs 
GROUP BY status;
"
```

Procurar:
- `pending` jobs stuck (> 30min old)
- `failed` jobs (check error_message)
- Orphan sessions (status != 'completed')

### 3. Check Logs

```bash
# Worker logs
grep "ERROR\|WARNING" /tmp/logs/Start_application_*.log | tail -50

# Memory tracking
grep "RAM\|CPU" /tmp/logs/Start_application_*.log | tail -20
```

### 4. Check Connection Pool

Se uploads falham com "connection pool exhausted":
- Verificar que `DatabasePool` usa 20 max connections
- Confirmar que `return_connection()` Ã© chamado em `finally`
- Validar que autocommit Ã© restaurado

---

## ğŸš€ Production Deployment Checklist

Antes de deploy para autoscale:

- [x] Connection pooling implementado (20 conns/worker)
- [x] Safeguards race-free (SERIALIZABLE transactions)
- [x] Memory streaming (fetchmany nÃ£o fetchall)
- [x] Heartbeat + timeout (30min auto-recovery)
- [x] Worker status endpoint (/api/worker/status)
- [ ] **Task 18 PASS** (regression test)
- [ ] **Task 19 PASS** (stress test)
- [ ] **Task 20 PASS** (soak test)

---

## ğŸ“ Notas

- **Test files:** Scripts criam ficheiros automaticamente em `tests/fixtures/`
- **Cleanup:** Testes perguntam se quer deletar ficheiros no final
- **Parallel execution:** NÃ£o executar mÃºltiplos testes simultaneamente
- **Production:** Adaptar BASE_URL para production deployment antes de testar

---

## ğŸ› Known Issues

Nenhum conhecido apÃ³s correÃ§Ãµes de autocommit (Oct 21, 2025).

Sistema production-ready para autoscale deployment! ğŸš€
